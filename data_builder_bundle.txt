
# BatchTrack Data Builder System - Complete Text Bundle
# Generated from data_builder/ directory structure

## TABLE OF CONTENTS
1. Main Module Files
2. Configuration and Data Files
3. Output Examples
4. Documentation

================================================================================
## 1. MAIN MODULE FILES
================================================================================

### data_builder/__init__.py
```python
"""Ingredient data builder modules."""

from . import ai_worker, compiler, database_manager, term_collector

__all__ = [
    "ai_worker",
    "compiler",
    "database_manager",
    "term_collector",
]
```

### data_builder/ingredients/__init__.py
```python
"""Ingredient data builder modules."""

from . import ai_worker, compiler, database_manager, term_collector

__all__ = [
    "ai_worker",
    "compiler",
    "database_manager",
    "term_collector",
]
```

### data_builder/ingredients/ai_worker.py
```python
"""OpenAI worker responsible for fetching structured ingredient data."""
from __future__ import annotations

import json
import logging
import os
import time
from typing import Any, Dict

import openai

LOGGER = logging.getLogger(__name__)

openai.api_key = os.environ.get("OPENAI_API_KEY")
if not openai.api_key:
    LOGGER.warning("OPENAI_API_KEY is not set; ai_worker will fail until configured.")

MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4-turbo-preview")
TEMPERATURE = float(os.getenv("OPENAI_TEMPERATURE", "0.1"))
MAX_RETRIES = int(os.getenv("AI_MAX_RETRIES", "3"))
RETRY_BACKOFF_SECONDS = float(os.getenv("AI_RETRY_BACKOFF", "3"))

INGREDIENT_CATEGORIES = [
    "Aqueous Solutions & Blends", 
    "Botanicals (Dried)",
    "Butters (Solid Fats)",
    "Clays",
    "Colorants & Pigments",
    "Cultures, SCOBYs & Fermentation",
    "Essential Oils",
    "Extracts (Alcohols & Solvents)",
    "Flours & Powders (Organic)",
    "Fragrance Oils",
    "Fruits, Nuts & Seeds (Whole/Chopped)",
    "Herbs & Spices (Dried Botanicals)",
    "Hydrosols & Floral Waters",
    "Liquid Extracts (Aqueous/Glycerine)",
    "Liquids (Aqueous)",
    "Miscellaneous",
    "Oils (Carrier & Fixed)",
    "Polymers (Synthetic)",
    "Preservatives & Additives",
    "Resins (Natural)",
    "Salts & Minerals",
    "Starches & Thickeners",
    "Sugars & Syrups",
    "Surfactants & Emulsifiers",
    "Waxes"
]

SYSTEM_PROMPT = (
    "You are IngredientCompilerGPT, an expert data extraction agent for artisanal "
    "manufacturing. You only output JSON that conforms to the provided schema."
)

JSON_SCHEMA_SPEC = r"""
Return JSON using this schema (all strings trimmed, booleans only true/false, lists sorted alphabetically):
{
  "ingredient": {
    "common_name": "string (required)",
    "category": "one of the approved ingredient categories",
    "botanical_name": "string",
    "inci_name": "string",
    "cas_number": "string",
    "short_description": "string",
    "detailed_description": "string",
    "origin": {
      "regions": ["string"],
      "source_material": "string",
      "processing_methods": ["Cold Pressed", "Solvent Extracted", ...]
    },
    "primary_functions": ["Emollient", "Humectant", "Bulking", ...],
    "regulatory_notes": ["IFRA safe", "Food grade", ...],
    "items": [
      {
        "item_name": "Shea Butter (Refined)",
        "physical_form": "choose from common forms such as Butter, Powder, Chips, Pellets, Granules, Crystals, Oil, Resin, Wax, Paste, Slab, Whole, Slices, Shavings, Ribbon, Nibs, Shreds, Flakes, Gel, Puree, Juice, Concentrate, Syrup",
        "synonyms": ["aka", ...],
        "applications": ["Soap", "Bath Bomb", "Chocolate", "Lotion", "Candle"],
        "function_tags": ["Stabilizer", "Fragrance", "Colorant", "Binder", "Fuel"],
        "safety_tags": ["Dermal Limit 3%", "Photosensitizer", ...],
          "shelf_life_days": 30-3650,
        "sds_hazards": ["Flammable", "Allergen"],
        "storage": {
          "temperature_celsius": {"min": 5, "max": 25},
          "humidity_percent": {"max": 60},
          "special_instructions": "Keep out of light"
        },
        "specifications": {
          "sap_naoh": 0.128,
          "sap_koh": 0.18,
          "iodine_value": 10,
          "melting_point_celsius": {"min": 30, "max": 35},
          "flash_point_celsius": 200,
          "ph_range": {"min": 5, "max": 7},
          "usage_rate_percent": {"leave_on_max": 5, "rinse_off_max": 15}
        },
        "sourcing": {
          "common_origins": ["Ghana", "Ivory Coast"],
          "certifications": ["Organic", "Fair Trade"],
          "supply_risks": ["Seasonal Harvest"],
          "sustainability_notes": "Tree nut resource"
        },
        "form_bypass": true | false  // when true, display should use item_name alone (e.g., Water, Ice)
      }
    ],
    "taxonomy": {
      "scent_profile": ["Fruity", "Nutty"],
      "color_profile": ["Ivory", "Translucent"],
      "texture_profile": ["Brittle", "Soft"],
      "compatible_processes": ["Cold Process Soap", "Hot Process", "Sugar Confectionery", "Emulsion", "Anhydrous Balm", "Pressed Powder", "Small Batch Brewing"],
      "incompatible_processes": ["High-heat frying"]
    },
    "documentation": {
      "references": [
        {
          "title": "USP Monograph",
          "url": "https://example.com",
          "notes": "Key regulatory reference"
        }
      ],
      "last_verified": "ISO-8601 date"
    }
  },
  "data_quality": {
    "confidence": 0-1 float,
    "caveats": ["string"]
  }
}
"""

ERROR_OBJECT = {"error": "Unable to return ingredient payload"}

PROMPT_TEMPLATE = """
You are an expert ingredient data compiler.

TASK: Build the complete ingredient dossier for: "{ingredient}".

CONTEXT:
- Audience: artisanal formulators in soap, confections, cosmetics, herbalism, fermentation, aromatherapy, and small-batch baking.
- Scope: RAW INGREDIENTS ONLY. Never include packaging, containers, utensils, or finished consumer goods.
- For each ingredient, enumerate every common PHYSICAL FORM (solid, powder, puree, resin, etc.) used in craft production. Create a dedicated `items` entry for every form.
- Populate every applicable attribute in the schema. Use "unknown" only when absolutely no data exists.
- Use authoritative references (USP, FCC, cosmetic suppliers, herbal materia medica) when citing specs.
- Use metric units. Shelf life must be expressed in DAYS (convert from months/years when needed). Temperature in Celsius.
- All string values must be clear, sentence case, and free of marketing fluff.
- Return strictly valid JSON (UTF-8, double quotes, no trailing commas). If you are uncertain, respond with {{"error": "explanation"}}.

CONTROLLED VOCAB REMINDERS:
- physical_form must be a short noun (e.g., "Powder", "Chips", "Pellets", "Whole", "Puree", "Pressed Cake").
- function_tags should draw from: Emollient, Humectant, Surfactant, Emulsifier, Preservative, Antioxidant, Colorant, Fragrance, Exfoliant, Thickener, Binder, Fuel, Hardener, Stabilizer, Plasticizer, Chelator, Buffer, Flavor, Sweetener, Bittering Agent, Fermentation Nutrient.
- applications should be chosen from: Cold Process Soap, Hot Process Soap, Melt & Pour Soap, Lotion, Cream, Balm, Serum, Scrub, Perfume, Candle, Wax Melt, Lip Balm, Chocolate, Confection, Baked Good, Beverage, Fermented Beverage, Herbal Tincture, Hydro-distillation, Bath Bomb, Shower Steamer, Haircare, Skincare, Deodorant, Cleaner, Detergent, Paint, Dye Bath.
- ingredient.category must be one of: {categories}

FORM & SOLUTION GUIDANCE:
- Always include dairy variants (e.g., whole milk, 2% milk, skim milk powder) and note their distinct forms.
- Include buffered/stock solutions (e.g., 50% Sodium Hydroxide Solution, 20% Potassium Carbonate Solution) when common in production.
- Essential oils, hydrosols, absolutes, CO2 extracts, glycerites, tinctures, macerations, and infusions should be represented as distinct forms under the parent ingredient.
- When an ingredient should display without a suffix (Water, Ice, Steam), set `form_bypass`=true so the interface shows just "Water" or "Ice" while still recording the underlying physical_form.
- When no better industry name exists, use a concise solution label such as "Potassium Carbonate Solution (20%)" or simply "Brine Solution".

SCHEMA (required):
{schema}

OUTPUT CONTRACT:
- Respond with a single JSON object adhering to the schema.
- If ingredient is out of scope or data unavailable, return {error_object} with a precise message.
"""


def _render_prompt(ingredient_name: str) -> str:
    return PROMPT_TEMPLATE.format(
        ingredient=ingredient_name.strip(),
        schema=JSON_SCHEMA_SPEC,
        error_object=json.dumps(ERROR_OBJECT),
        categories=", ".join(INGREDIENT_CATEGORIES),
    )


def get_ingredient_data(ingredient_name: str) -> Dict[str, Any]:
    """Fetch structured data for a single ingredient via the OpenAI API."""

    if not ingredient_name or not ingredient_name.strip():
        raise ValueError("ingredient_name is required")

    if not openai.api_key:
        raise RuntimeError("OPENAI_API_KEY environment variable is not configured")

    user_prompt = _render_prompt(ingredient_name)
    last_error: Exception | None = None

    client = openai.OpenAI(api_key=openai.api_key)

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                temperature=TEMPERATURE,
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
            )
            content = response.choices[0].message.content
            if not content or not content.strip():
                raise ValueError("OpenAI returned empty response content")
            
            content = content.strip()
            LOGGER.debug("OpenAI response content: %s", content[:200] + "..." if len(content) > 200 else content)
            
            payload = json.loads(content)
            if not isinstance(payload, dict):
                raise ValueError("AI response was not a JSON object")
            return payload
        except json.JSONDecodeError as exc:
            last_error = exc
            LOGGER.warning("JSON decoding failed for %s (attempt %s): %s", ingredient_name, attempt, exc)
            LOGGER.warning("Raw content (first 200 chars): %s", content[:200] if 'content' in locals() else "No content available")
        except Exception as exc:  # pylint: disable=broad-except
            last_error = exc
            LOGGER.warning("OpenAI call failed for %s (attempt %s): %s", ingredient_name, attempt, exc)

        time.sleep(RETRY_BACKOFF_SECONDS * attempt)

    error_message = (
        f"Failed to compile ingredient '{ingredient_name}' after {MAX_RETRIES} attempts: {last_error}"
    )
    LOGGER.error(error_message)
    return {"error": error_message}
```

### data_builder/ingredients/compiler.py
```python
"""Iterative orchestrator that builds the ingredient library one record at a time."""
from __future__ import annotations

import argparse
import json
import logging
import os
import re
import sys
import time
from pathlib import Path
from typing import Any, Dict, Iterable, List, MutableMapping, Set

try:  # pragma: no cover - fallback for direct script execution
    from . import ai_worker, database_manager
except ImportError:  # pragma: no cover
    import ai_worker  # type: ignore
    import database_manager  # type: ignore

LOGGER = logging.getLogger("data_builder.ingredients.compiler")
BASE_DIR = Path(__file__).resolve().parent
OUTPUT_DIR = BASE_DIR / "output"
INGREDIENT_DIR = OUTPUT_DIR / "ingredients"
PHYSICAL_FORMS_FILE = OUTPUT_DIR / "physical_forms.json"
TAXONOMY_FILE = OUTPUT_DIR / "taxonomies.json"
DEFAULT_TERMS_FILE = BASE_DIR / "terms.json"
DEFAULT_SLEEP_SECONDS = float(os.getenv("COMPILER_SLEEP_SECONDS", "3"))


def slugify(value: str) -> str:
    """Generate a filesystem-safe slug."""

    slug = re.sub(r"[^a-zA-Z0-9]+", "_", value.lower()).strip("_")
    return slug or "ingredient"


def ensure_output_dirs() -> None:
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    INGREDIENT_DIR.mkdir(parents=True, exist_ok=True)


def _load_json_list(path: Path) -> Set[str]:
    if not path.exists():
        return set()
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        if isinstance(data, list):
            return {str(item) for item in data if str(item).strip()}
    except json.JSONDecodeError:
        LOGGER.warning("Failed to parse %s; regenerating.", path)
    return set()


def _write_json_list(path: Path, values: Iterable[str]) -> None:
    sorted_values = sorted({value for value in values if value})
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(sorted_values, indent=2), encoding="utf-8")


def _load_taxonomy_map() -> MutableMapping[str, Set[str]]:
    if not TAXONOMY_FILE.exists():
        return {}
    try:
        data = json.loads(TAXONOMY_FILE.read_text(encoding="utf-8"))
        return {key: set(value) for key, value in data.items() if isinstance(value, list)}
    except json.JSONDecodeError:
        LOGGER.warning("Failed to parse %s; regenerating.", TAXONOMY_FILE)
        return {}


def _write_taxonomy_map(values: MutableMapping[str, Set[str]]) -> None:
    serialized = {key: sorted(list(val_set)) for key, val_set in values.items() if val_set}
    TAXONOMY_FILE.parent.mkdir(parents=True, exist_ok=True)
    TAXONOMY_FILE.write_text(json.dumps(serialized, indent=2), encoding="utf-8")


def validate_payload(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Guard against malformed AI responses."""

    ingredient = payload.get("ingredient")
    if not isinstance(ingredient, dict):
        raise ValueError("Payload missing top-level 'ingredient' object")

    common_name = ingredient.get("common_name")
    if not common_name or not isinstance(common_name, str):
        raise ValueError("Ingredient is missing a valid 'common_name'")

    items = ingredient.get("items")
    if not isinstance(items, list) or not items:
        raise ValueError("Ingredient must include at least one item form")

    return ingredient


def update_lookup_files(payload: Dict[str, Any]) -> None:
    """Refresh the supporting lookup files for physical forms and taxonomies."""

    ingredient = payload.get("ingredient", {})
    items: List[Dict[str, Any]] = ingredient.get("items", []) or []

    # Physical forms
    existing_forms = _load_json_list(PHYSICAL_FORMS_FILE)
    for item in items:
        form = item.get("physical_form")
        if isinstance(form, str) and form.strip():
            existing_forms.add(form.strip())
    _write_json_list(PHYSICAL_FORMS_FILE, existing_forms)

    taxonomy_values = _load_taxonomy_map()

    def _extend_taxonomy(key: str, values: Iterable[str]) -> None:
        bucket = taxonomy_values.setdefault(key, set())
        for value in values:
            if isinstance(value, str) and value.strip():
                bucket.add(value.strip())

    # Item level tags
    for item in items:
        _extend_taxonomy("function_tags", item.get("function_tags", []) or [])
        _extend_taxonomy("applications", item.get("applications", []) or [])
        _extend_taxonomy("safety_tags", item.get("safety_tags", []) or [])

    # Top-level taxonomy dictionary if present
    taxonomy_obj = ingredient.get("taxonomy", {}) or {}
    for key, values in taxonomy_obj.items():
        if isinstance(values, list):
            _extend_taxonomy(key, values)

    _write_taxonomy_map(taxonomy_values)


def save_payload(payload: Dict[str, Any], slug: str) -> Path:
    target = INGREDIENT_DIR / f"{slug}.json"
    target.parent.mkdir(parents=True, exist_ok=True)
    target.write_text(json.dumps(payload, indent=2, sort_keys=True), encoding="utf-8")
    return target


def process_next_term(sleep_seconds: float, min_priority: int) -> bool:
    """Process a single pending task honoring the priority floor. Returns False when queue is empty."""

    task = database_manager.get_next_pending_task(min_priority=min_priority)
    if not task:
        LOGGER.info("No pending tasks found at priority >= %s; compiler is finished.", min_priority)
        return False

    term, priority = task
    LOGGER.info("Processing term: %s (priority %s)", term, priority)
    database_manager.update_task_status(term, "processing")

    try:
        payload = ai_worker.get_ingredient_data(term)
        if not isinstance(payload, dict) or payload.get("error"):
            raise RuntimeError(payload.get("error") if isinstance(payload, dict) else "Unknown AI failure")

        ingredient = validate_payload(payload)
        slug = slugify(ingredient.get("common_name", term))
        save_payload(payload, slug)
        update_lookup_files(payload)
        database_manager.update_task_status(term, "completed")
        LOGGER.info("Successfully saved %s -> %s", term, slug)
    except Exception as exc:  # pylint: disable=broad-except
        database_manager.update_task_status(term, "error")
        LOGGER.exception("Failed to process %s: %s", term, exc)

    time.sleep(sleep_seconds)
    return True


def run_compiler(sleep_seconds: float, max_ingredients: int | None, min_priority: int) -> None:
    ensure_output_dirs()
    iterations = 0
    while True:
        if max_ingredients and iterations >= max_ingredients:
            LOGGER.info("Reached ingredient cap (%s); stopping.", max_ingredients)
            break
        if not process_next_term(sleep_seconds, min_priority):
            break
        iterations += 1


def parse_args(argv: List[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Iteratively compile ingredient data via OpenAI")
    parser.add_argument("--terms-file", default=str(DEFAULT_TERMS_FILE), help="Seed term file (JSON array of {term, priority})")
    parser.add_argument("--sleep-seconds", type=float, default=DEFAULT_SLEEP_SECONDS, help="Delay between API calls")
    parser.add_argument("--max-ingredients", type=int, default=0, help="Optional cap for number of processed ingredients in this run")
    parser.add_argument("--min-priority", type=int, default=database_manager.MIN_PRIORITY, help="Minimum priority (1-10) required to process a queued ingredient")
    return parser.parse_args(argv)


def main(argv: List[str] | None = None) -> None:
    logging.basicConfig(
        level=os.getenv("COMPILER_LOG_LEVEL", "INFO"),
        format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
    )

    args = parse_args(argv or sys.argv[1:])

    # Initialize queue if needed
    try:
        database_manager.initialize_queue(args.terms_file)
    except FileNotFoundError:
        LOGGER.warning("Terms file %s not found; queue will only use existing entries.", args.terms_file)

    run_compiler(
        sleep_seconds=args.sleep_seconds,
        max_ingredients=args.max_ingredients or None,
        min_priority=max(database_manager.MIN_PRIORITY, min(args.min_priority, database_manager.MAX_PRIORITY)),
    )


if __name__ == "__main__":
    main()
```

### data_builder/ingredients/database_manager.py
```python
"""SQLite-backed task queue utilities for the iterative compiler."""
from __future__ import annotations

import json
import logging
import os
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

from sqlalchemy import Column, DateTime, Integer, String, create_engine, select, text
from sqlalchemy.orm import Session, declarative_base, sessionmaker

LOGGER = logging.getLogger(__name__)

BASE_DIR = Path(__file__).resolve().parent
DEFAULT_DB_PATH = BASE_DIR / "compiler_state.db"
DB_PATH = Path(os.environ.get("COMPILER_DB_PATH", DEFAULT_DB_PATH))
DATABASE_URL = f"sqlite:///{DB_PATH}"

engine = create_engine(
    DATABASE_URL,
    connect_args={"check_same_thread": False},
    future=True,
)
SessionLocal = sessionmaker(bind=engine, expire_on_commit=False, future=True)
Base = declarative_base()


DEFAULT_PRIORITY = 5
MIN_PRIORITY = 1
MAX_PRIORITY = 10


class TaskQueue(Base):
    """ORM model representing a queued ingredient term."""

    __tablename__ = "task_queue"

    term = Column(String, primary_key=True)
    status = Column(String, nullable=False, default="pending")
    last_updated = Column(DateTime, nullable=False, default=datetime.utcnow)
    priority = Column(Integer, nullable=False, default=DEFAULT_PRIORITY)


VALID_STATUSES = {"pending", "processing", "completed", "error"}


def ensure_tables_exist() -> None:
    """Create the task_queue table if it does not already exist."""

    Base.metadata.create_all(engine)
    _ensure_priority_column()


def _ensure_priority_column() -> None:
    with engine.connect() as conn:
        columns = conn.execute(text("PRAGMA table_info(task_queue)")).fetchall()
        column_names = {row[1] for row in columns}
        if "priority" not in column_names:
            conn.execute(
                text("ALTER TABLE task_queue ADD COLUMN priority INTEGER NOT NULL DEFAULT :default"),
                {"default": DEFAULT_PRIORITY},
            )
            LOGGER.info("Added priority column to task_queue")


@contextmanager
def get_session() -> Iterable[Session]:
    """Provide a transactional scope around a series of operations."""

    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()


def _sanitize_priority(value) -> int:
    try:
        priority = int(value)
    except (TypeError, ValueError):
        return DEFAULT_PRIORITY
    return max(MIN_PRIORITY, min(MAX_PRIORITY, priority))


def _load_terms(terms_file_path: Path) -> List[Tuple[str, int]]:
    """Read the seed term list from JSON or newline-delimited text, including priority metadata."""

    if not terms_file_path.exists():
        raise FileNotFoundError(f"Seed terms file not found: {terms_file_path}")

    raw_text = terms_file_path.read_text(encoding="utf-8").strip()
    if not raw_text:
        return []

    entries: dict[str, int] = {}

    # Attempt JSON first (expecting an array of objects or strings)
    try:
        data = json.loads(raw_text)
        if isinstance(data, list):
            for item in data:
                if isinstance(item, str):
                    term = item.strip()
                    if term:
                        entries[term] = max(entries.get(term, MIN_PRIORITY), DEFAULT_PRIORITY)
                elif isinstance(item, dict):
                    term = str(item.get("term") or item.get("name") or "").strip()
                    if not term:
                        continue
                    priority = _sanitize_priority(item.get("priority") or item.get("priority_score"))
                    existing = entries.get(term)
                    if existing is None or priority > existing:
                        entries[term] = priority
        else:
            raise json.JSONDecodeError("Not a list", raw_text, 0)
    except json.JSONDecodeError:
        # Fallback to newline-delimited text
        for line in raw_text.splitlines():
            term = line.strip()
            if term:
                entries[term] = max(entries.get(term, MIN_PRIORITY), DEFAULT_PRIORITY)

    deduped_sorted_terms = sorted(entries.items(), key=lambda item: item[0])
    return deduped_sorted_terms


def initialize_queue(terms_file_path: str | os.PathLike[str]) -> int:
    """Populate the queue with seed terms.

    Args:
        terms_file_path: Path to a JSON array or newline-delimited list of terms.

    Returns:
        Number of terms inserted.
    """

    ensure_tables_exist()
    terms_path = Path(terms_file_path)
    term_entries = _load_terms(terms_path)
    if not term_entries:
        LOGGER.warning("No terms found in %s; queue not modified.", terms_path)
        return 0

    inserted = 0
    with get_session() as session:
        existing_rows = {
            row[0]: row[1]
            for row in session.execute(select(TaskQueue.term, TaskQueue.priority))
        }
        for term, priority in term_entries:
            normalized_priority = _sanitize_priority(priority)
            existing_priority = existing_rows.get(term)
            if existing_priority is not None:
                if normalized_priority > existing_priority:
                    session.query(TaskQueue).filter(TaskQueue.term == term).update(
                        {"priority": normalized_priority, "last_updated": datetime.utcnow()}
                    )
                    LOGGER.debug("Elevated priority for %s from %s to %s", term, existing_priority, normalized_priority)
                continue
            session.add(
                TaskQueue(
                    term=term,
                    status="pending",
                    last_updated=datetime.utcnow(),
                    priority=normalized_priority,
                )
            )
            inserted += 1

    if inserted:
        LOGGER.info("Inserted %s new terms into the queue.", inserted)
    else:
        LOGGER.info("Queue already contained all provided terms; nothing inserted.")
    return inserted


def get_next_pending_task(min_priority: int = MIN_PRIORITY) -> Optional[tuple[str, int]]:
    """Return the next pending term honoring priority sorting."""

    ensure_tables_exist()
    with get_session() as session:
        task: Optional[TaskQueue] = (
            session.query(TaskQueue)
            .filter(
                TaskQueue.status == "pending",
                TaskQueue.priority >= max(MIN_PRIORITY, min_priority),
            )
            .order_by(TaskQueue.priority.desc(), TaskQueue.term.asc())
            .first()
        )
        if task:
            return task.term, task.priority
        return None


def update_task_status(term: str, status: str) -> None:
    """Update the status of a queue item."""

    if status not in VALID_STATUSES:
        raise ValueError(f"Invalid status '{status}'. Valid options: {sorted(VALID_STATUSES)}")

    ensure_tables_exist()
    with get_session() as session:
        task: Optional[TaskQueue] = session.get(TaskQueue, term)
        if not task:
            raise LookupError(f"Task '{term}' does not exist in the queue.")
        task.status = status
        task.last_updated = datetime.utcnow()


def queue_is_empty() -> bool:
    """Return True if there are no records in the queue."""

    ensure_tables_exist()
    with get_session() as session:
        exists = session.query(TaskQueue.term).first()
        return exists is None


def get_queue_summary() -> dict:
    """Return a count of tasks grouped by status for observability."""

    ensure_tables_exist()
    summary = {status: 0 for status in VALID_STATUSES}
    with get_session() as session:
        rows = session.execute(
            select(TaskQueue.status)
        )
        for (status,) in rows:
            summary[status] = summary.get(status, 0) + 1
    summary["total"] = sum(summary.values())
    return summary
```

### data_builder/ingredients/term_collector.py
```python
"""AI-assisted collector that builds the seed ingredient queue and lookup forms."""
from __future__ import annotations

import argparse
import json
import logging
import os
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

import openai

LOGGER = logging.getLogger(__name__)
BASE_DIR = Path(__file__).resolve().parent
OUTPUT_DIR = BASE_DIR / "output"
TERMS_FILE = BASE_DIR / "terms.json"
PHYSICAL_FORMS_FILE = OUTPUT_DIR / "physical_forms.json"
openai.api_key = os.environ.get("OPENAI_API_KEY")
if not openai.api_key:
    LOGGER.warning("OPENAI_API_KEY is not set; term_collector will run in repository-only mode unless --skip-ai is provided.")
MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4-turbo-preview")
TEMPERATURE = float(os.getenv("OPENAI_TEMPERATURE", "0.2"))
MAX_BATCH_RETRIES = int(os.getenv("AI_MAX_RETRIES", "3"))
DEFAULT_PRIORITY = 5
SEED_PRIORITY = 7
MIN_PRIORITY = 1
MAX_PRIORITY = 10

BASE_PHYSICAL_FORMS: Set[str] = {
    "Whole", "Slices", "Diced", "Chopped", "Minced", "Crushed", "Ground",
    "Powder", "Microfine Powder", "Granules", "Flakes", "Shreds", "Ribbons",
    "Pellets", "Chips", "Nib", "Buds", "Flowers", "Leaves", "Needles",
    "Stems", "Bark", "Resin", "Gum", "Latex", "Sap", "Juice", "Puree",
    "Paste", "Concentrate", "Syrup", "Molasses", "Infusion", "Decoction",
    "Tincture", "Hydrosol", "Essential Oil", "Absolute", "CO2 Extract",
    "Oleoresin", "Distillate", "Cold Pressed Oil", "Refined Oil", "Butter",
    "Wax", "Wafers", "Slab", "Block", "Sheet", "Beads", "Crystals",
    "Isolate", "Powdered Extract", "Spray-dried Powder", "Freeze-dried",
    "Dehydrated", "Whole Dried", "Roasted", "Toasted", "Smoke-dried",
    "Fermented", "Macerated", "Steeped", "Infused Oil", "Pressed Cake",
    "Expeller Cake", "Pomace", "Fiber", "Husk", "Hull", "Seed",
    "Kernel", "Nutmeat", "Flour", "Meal", "Starch", "Proteinate",
    "Chelated Powder", "Soluble Powder", "Emulsion", "Gel", "Jelly",
    "Glycerite", "Vinegar Extract", "Alcohol Extract", "Supercritical Extract"
}

EXAMPLE_INGREDIENTS: List[str] = [
    "Acacia Gum",
    "Activated Charcoal",
    "Agar Agar",
    "Alkanet Root",
    "Aloe Vera",
    "Apricot Kernel Oil",
    "Arrowroot Powder",
    "Beeswax",
    "Bentonite Clay",
    "Blue Cornflower",
    "Calendula",
    "Candelilla Wax",
    "Cane Sugar",
    "Cocoa Butter",
    "Coconut Oil",
    "Epsom Salt",
    "Gluconic Acid",
    "Glycerin",
    "Grapefruit Essential Oil",
    "Green Tea",
    "Honey Powder",
    "Jojoba Oil",
    "Kaolin Clay",
    "Kombucha Starter",
    "Lanolin",
    "Lavender",
    "Lye Solution (50% NaOH)",
    "Madder Root",
    "Magnesium Hydroxide",
    "Neem Oil",
    "Orange Peel",
    "Pink Himalayan Salt",
    "Potassium Carbonate Solution",
    "Propolis",
    "Rosemary Oleoresin",
    "Sassafras Bark",
    "Sea Buckthorn Pulp",
    "Shea Butter",
    "Sodium Bicarbonate",
    "Soy Lecithin",
    "Stearic Acid",
    "Tapioca Starch",
    "Turmeric",
    "Vanilla Bean",
    "White Willow Bark",
    "Witch Hazel Distillate",
    "Xanthan Gum",
    "Ylang Ylang Essential Oil",
    "Zinc Oxide",
]

EXAMPLE_PHYSICAL_FORMS: Set[str] = {
    "Essential Oil",
    "CO2 Extract",
    "Absolute",
    "Hydrosol",
    "Infusion",
    "Decoction",
    "Tincture",
    "Oil Infusion",
    "Pressed Cake",
    "Macreate",
    "Lye Solution",
    "Stock Solution",
}

SYSTEM_PROMPT = (
    "You are IngredientLibrarianGPT, a research assistant who curates canonical "
    "raw ingredients for small-batch makers across soap, cosmetic, confectionery, "
    "baking, beverage, herbal, aromatherapy, candle, and fermentation domains."
)

TERMS_PROMPT = """
You build the authoritative master ingredient index.

DEFINITIONS:
- An INGREDIENT (aka base) is the abstract raw material such as "Lavender", "Shea Butter", "Cane Sugar", "Citric Acid".
- An ITEM is the combination of INGREDIENT + PHYSICAL FORM (e.g., Lavender Buds, Lavender Essential Oil). ONLY return base ingredient names here.
- Focus on botanicals, minerals, clays, waxes, fats, sugars, acids, fermentation adjuncts, resins, essential oils, extracts, isolates, and other raw materials used by small-batch makers.
- EXCLUDE: packaging, containers, utensils, finished products, synthetic fragrances without a backing raw material, equipment, and vague marketing terms.

TASK:
Return a strictly alphabetical list of NEW base ingredient names that have not appeared previously. Every entry must be unique, discoverable in supplier catalogs, and relevant to at least one target industry: soapmaking, personal care, artisan food & beverage, herbal apothecary, candles, cosmetics, confectionery, or fermentation.

SOLUTION & EXTRACT GUIDANCE:
- Include buffered solutions, stock lye solutions (e.g., 50% NaOH), mineral brines, herbal glycerites, tinctures, vinegars, and other make-ready raw solutions when they are handled as ingredients.
- For botanicals with essential oils, hydrosols, absolutes, CO2 extracts, etc., treat the plant as the ingredient and enumerate those forms inside `common_forms`.
- Treat {count} as a minimum for this batch. If more valid ingredients exist beyond that count, KEEP ADDING them until you naturally reach the response limit. The overarching library will eventually exceed 5,000 and may reach 15,000+, so never intentionally leave alphabetical gaps.

For each ingredient, list the most common PHYSICAL FORMS (include essential oil/extract variants when applicable).

CONSTRAINTS:
- Provide AT LEAST {count} ingredient records (more is welcome if available).
- The first ingredient must be lexicographically GREATER than "{start_after}".
- Alphabetize A-Z.
- Avoid duplicates of the provided examples.
- Use concise proper names (e.g., "Calendula", "Magnesium Hydroxide").
- Assign a relevance score from 1-10 (integer) where 10 = essential for small-batch makers and 1 = niche or situational.
- If absolutely no valid ingredients remain, return an empty list.

OUTPUT FORMAT:
Return JSON only:
{{
  "ingredients": [
    {{
      "name": "string",
      "category": "one of: Botanical, Mineral, Animal-Derived, Fermentation, Chemical, Resin, Wax, Fatty Acid, Sugar, Acid, Salt, Aroma",
      "industries": ["Soap", "Cosmetic", "Candle", "Confection", "Beverage", "Herbal", "Baking", "Fermentation", "Aromatherapy"],
      "common_forms": ["Powder", "Essential Oil", ...],
      "notes": "1-sentence rationale",
      "priority_score": 1-10 integer (10 = highest relevance/urgency for makers)
    }}
  ],
  "physical_forms": ["unique physical forms referenced"]
}}

EXAMPLES TO EMULATE:
{examples}
"""


class TermCollector:
    """Coordinates local seeding, AI expansion, and output writing."""

    def __init__(
        self,
        target_count: int,
        batch_size: int,
        include_ai: bool = True,
        seed_root: Optional[Path] = None,
        example_file: Optional[Path] = None,
    ) -> None:
        self.seed_root = seed_root if seed_root and seed_root.exists() else None
        self.target_count = target_count
        self.batch_size = batch_size
        self.include_ai = include_ai and bool(openai.api_key)
        self.terms: Dict[str, int] = {}
        self.physical_forms: Set[str] = set(BASE_PHYSICAL_FORMS) | EXAMPLE_PHYSICAL_FORMS
        self.prompt_examples: Set[str] = set(EXAMPLE_INGREDIENTS)
        if example_file and example_file.exists():
            try:
                data = json.loads(example_file.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    self.prompt_examples.update(str(item).strip() for item in data if str(item).strip())
            except json.JSONDecodeError:
                LOGGER.warning("Example file %s is not valid JSON; ignoring", example_file)

    def _extract_priority(self, value) -> int:
        try:
            priority = int(value)
        except (TypeError, ValueError):
            priority = DEFAULT_PRIORITY
        return max(MIN_PRIORITY, min(MAX_PRIORITY, priority))

    def _register_term(self, term: str, priority: int) -> bool:
        """Track a term with the highest observed priority. Returns True if newly added."""
        cleaned = term.strip()
        if not cleaned:
            return False
        normalized_priority = self._extract_priority(priority)
        existing = self.terms.get(cleaned)
        if existing is None or normalized_priority > existing:
            self.terms[cleaned] = normalized_priority
            return existing is None
        return False

    def slugify(self, value: str) -> str:
        """
        Converts a string into a "slug".

        A slug is a readable, URL-friendly identifier.
        """
        value = str(value)
        value = re.sub(r'[^\w\s-]', '', value).strip().lower()
        value = re.sub(r'[-\s]+', '-', value)
        return value

    # ------------------------------------------------------------------
    # Seed extraction
    # ------------------------------------------------------------------
    def ingest_seed_directory(self) -> None:
        if not self.seed_root:
            LOGGER.info("No seed directory supplied; relying entirely on AI generation.")
            return

        for json_path in self.seed_root.rglob("*.json"):
            try:
                data = json.loads(json_path.read_text(encoding="utf-8"))
            except json.JSONDecodeError:
                LOGGER.debug("Skipping non-JSON file %s", json_path)
                continue
            self._collect_from_obj(data)

        LOGGER.info("Loaded %s seed terms from %s", len(self.terms), self.seed_root)

    def _collect_from_obj(self, obj: Any) -> None:
        if isinstance(obj, dict):
            for key, value in obj.items():
                if key == "category_name":
                    continue
                if key in {"name", "common_name", "ingredient_name"} and isinstance(value, str):
                    cleaned = value.strip()
                    if cleaned:
                        self._register_term(cleaned, SEED_PRIORITY)
                if key == "ingredient" and isinstance(value, dict):
                    name = value.get("name")
                    if isinstance(name, str) and name.strip():
                        self._register_term(name.strip(), SEED_PRIORITY)
                if key == "physical_form" and isinstance(value, str):
                    self.physical_forms.add(value.strip())
                self._collect_from_obj(value)
        elif isinstance(obj, list):
            for item in obj:
                self._collect_from_obj(item)

    # ------------------------------------------------------------------
    # AI Expansion
    # ------------------------------------------------------------------
    def expand_with_ai(self) -> None:
        if not self.include_ai:
            LOGGER.warning("OPENAI_API_KEY missing; skipping AI term generation")
            return

        while len(self.terms) < self.target_count:
            start_after = sorted(self.terms.keys())[-1] if self.terms else ""
            batch_size = min(self.batch_size, self.target_count - len(self.terms))
            LOGGER.info(
                "Requesting %s new terms after '%s' (current total: %s/%s)",
                batch_size,
                start_after or "<start>",
                len(self.terms),
                self.target_count,
            )
            examples = self._compose_examples(24)
            payload = self._request_ai_batch(batch_size, start_after, examples)
            if not payload:
                LOGGER.warning("AI returned no payload; breaking out")
                break
            new_terms = 0
            for record in payload.get("ingredients", []):
                name = record.get("name")
                if not isinstance(name, str):
                    continue
                cleaned = name.strip()
                if not cleaned:
                    continue
                priority = record.get("priority_score")
                if self._register_term(cleaned, priority):
                    new_terms += 1
                for form in record.get("common_forms", []) or []:
                    if isinstance(form, str) and form.strip():
                        self.physical_forms.add(form.strip())
            for form in payload.get("physical_forms", []) or []:
                if isinstance(form, str) and form.strip():
                    self.physical_forms.add(form.strip())
            LOGGER.info("Ingested %s brand-new ingredients", new_terms)
            if new_terms == 0:
                break

    def _compose_examples(self, limit: int) -> List[str]:
        live_samples = sorted(self.terms.keys())[: limit // 2]
        curated = sorted(self.prompt_examples)[: limit - len(live_samples)]
        combined = curated + live_samples
        return combined[:limit]

    def _request_ai_batch(self, count: int, start_after: str, examples: List[str]) -> Dict[str, Any]:
        user_prompt = TERMS_PROMPT.format(
            count=count,
            start_after=start_after.replace("\"", ""),
            examples=json.dumps(examples, indent=2) if examples else "[]",
        )

        client = openai.OpenAI(api_key=openai.api_key)

        for attempt in range(1, MAX_BATCH_RETRIES + 1):
            try:
                response = client.chat.completions.create(
                    model=MODEL_NAME,
                    temperature=TEMPERATURE,
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": user_prompt},
                    ],
                )
                content = response.choices[0].message.content
                if not content or not content.strip():
                    raise ValueError("OpenAI returned empty response content")

                content = content.strip()
                LOGGER.debug("OpenAI response content: %s", content[:200] + "..." if len(content) > 200 else content)

                payload = json.loads(content)
                if not isinstance(payload, dict):
                    raise ValueError("AI payload is not a JSON object")
                return payload
            except json.JSONDecodeError as exc:
                LOGGER.warning("JSON decode failed for attempt %s. Raw content (first 200 chars): %s", attempt, content[:200] if 'content' in locals() else "No content available")
                LOGGER.warning("JSON decode error: %s", exc)
            except Exception as exc:  # pylint: disable=broad-except
                LOGGER.warning("Term generation attempt %s failed: %s", attempt, exc)
        return {}

    # ------------------------------------------------------------------
    # Outputs
    # ------------------------------------------------------------------
    def write_terms_file(self, path: Path = TERMS_FILE) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        payload = [
            {"term": term, "priority": priority}
            for term, priority in sorted(self.terms.items(), key=lambda item: (-item[1], item[0]))
        ]
        path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        LOGGER.info("Wrote %s prioritized terms to %s", len(payload), path)

    def write_forms_file(self, path: Path = PHYSICAL_FORMS_FILE) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        ordered = sorted(self.physical_forms)
        path.write_text(json.dumps(ordered, indent=2), encoding="utf-8")
        LOGGER.info("Wrote %s physical forms to %s", len(ordered), path)

    


def parse_args(argv: List[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Generate the ingredient term queue via local + AI sources")
    parser.add_argument("--seed-root", default="", help="Optional directory to scan for seed data (leave blank to skip)")
    parser.add_argument("--example-file", default="", help="Optional JSON array of canonical example ingredients for prompt context")
    parser.add_argument("--target-count", type=int, default=5000, help="Desired total number of base ingredients")
    parser.add_argument("--batch-size", type=int, default=250, help="AI batch size per request")
    parser.add_argument("--terms-file", default=str(TERMS_FILE), help="Destination JSON file for term list (term + priority)")
    parser.add_argument("--forms-file", default=str(PHYSICAL_FORMS_FILE), help="Destination file for physical forms list")
    parser.add_argument("--skip-ai", action="store_true", help="Only use repo seeds; do not call the AI API")
    return parser.parse_args(argv)


def main(argv: List[str] | None = None) -> None:
    logging.basicConfig(
        level=os.getenv("COMPILER_LOG_LEVEL", "INFO"),
        format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
    )
    args = parse_args(argv)

    collector = TermCollector(
        target_count=args.target_count,
        batch_size=args.batch_size,
        include_ai=not args.skip_ai,
        seed_root=Path(args.seed_root).resolve() if args.seed_root else None,
        example_file=Path(args.example_file).resolve() if args.example_file else None,
    )

    collector.ingest_seed_directory()
    collector.expand_with_ai()
    collector.write_terms_file(Path(args.terms_file))
    collector.write_forms_file(Path(args.forms_file))


if __name__ == "__main__":
    main()
```

### data_builder/ingredients/sources.py
```python
"""Helper utilities for enriching ingredient data from external sources."""
from __future__ import annotations

import csv
import json
import logging
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests

LOGGER = logging.getLogger(__name__)

BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data_sources"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# ---------------------------------------------------------------------------
# Environment variable keys
# ---------------------------------------------------------------------------
PUBCHEM_API_KEY = os.getenv("PUBCHEM_API_KEY")
USDA_API_KEY = os.getenv("USDA_API_KEY")
EWG_API_KEY = os.getenv("EWG_API_KEY")
TGSC_API_KEY = os.getenv("TGSC_API_KEY")
NHP_API_KEY = os.getenv("NHP_API_KEY")  # Health Canada natural health products

COSING_CSV_PATH = Path(os.getenv("COSING_CSV_PATH", DATA_DIR / "cosing.csv"))
HSCG_CSV_PATH = Path(os.getenv("HSCG_CSV_PATH", DATA_DIR / "hscg_ingredients.csv"))
TGSC_CSV_PATH = Path(os.getenv("TGSC_CSV_PATH", DATA_DIR / "tgsc_ingredients.csv"))
NHP_JSON_PATH = Path(os.getenv("NHP_JSON_PATH", DATA_DIR / "health_canada_nhp.json"))

SOURCE_ORDER = [
    "pubchem",
    "cosing",
    "usda",
    "hscg",
    "ewg",
    "tgsc",
    "health_canada_nhp",
]


@dataclass
class SourcePayload:
    source: str
    data: Dict[str, Any]


class IngredientSourceBroker:
    """Attempts to gather structured facts from configured sources."""

    def __init__(self) -> None:
        self.session = requests.Session()

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    def gather(self, term: str) -> Dict[str, Any]:
        payload: Dict[str, Any] = {}
        for source_name in SOURCE_ORDER:
            try:
                handler = getattr(self, f"_fetch_{source_name}")
            except AttributeError:
                LOGGER.debug("No handler for source %s", source_name)
                continue
            result = handler(term)
            if not result:
                continue
            payload[source_name] = result.data
        return payload

    # ------------------------------------------------------------------
    # Individual source handlers
    # ------------------------------------------------------------------
    def _fetch_pubchem(self, term: str) -> Optional[SourcePayload]:
        endpoint = "https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{term}/property/MolecularWeight,ExactMass,Density,BoilingPoint,FlashPoint,InChIKey,CanonicalSMILES/JSON"
        try:
            response = self.session.get(endpoint.format(term=requests.utils.quote(term)), timeout=10)
            response.raise_for_status()
            blob = response.json()
            props = blob.get("PropertyTable", {}).get("Properties", [])
            if not props:
                return None
            data = props[0]
            if PUBCHEM_API_KEY:
                data["api_key_used"] = True
            return SourcePayload(source="pubchem", data=data)
        except Exception as exc:  # pylint: disable=broad-except
            LOGGER.debug("PubChem lookup failed for %s: %s", term, exc)
            return None

    def _fetch_cosing(self, term: str) -> Optional[SourcePayload]:
        if not COSING_CSV_PATH.exists():
            return None
        term_lower = term.lower()
        try:
            with COSING_CSV_PATH.open(encoding="utf-8") as handle:
                reader = csv.DictReader(handle)
                for row in reader:
                    names = [row.get("INCI Name", ""), row.get("Synonyms", "")]
                    if any(term_lower == (name or "").strip().lower() for name in names):
                        return SourcePayload(source="cosing", data=row)
        except Exception as exc:  # pylint: disable=broad-except
            LOGGER.debug("CosIng CSV lookup failed for %s: %s", term, exc)
        return None

    def _fetch_usda(self, term: str) -> Optional[SourcePayload]:
        api_key = USDA_API_KEY
        if not api_key:
            return None
        try:
            url = "https://api.nal.usda.gov/fdc/v1/foods/search"
            params = {"query": term, "pageSize": 1, "api_key": api_key}
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            blob = response.json()
            foods = blob.get("foods", [])
            if not foods:
                return None
            food = foods[0]
            return SourcePayload(source="usda", data=food)
        except Exception as exc:  # pylint: disable=broad-except
            LOGGER.debug("USDA lookup failed for %s: %s", term, exc)
            return None

    def _fetch_hscg(self, term: str) -> Optional[SourcePayload]:
        if not HSCG_CSV_PATH.exists():
            return None
        term_lower = term.lower()
        try:
            with HSCG_CSV_PATH.open(encoding="utf-8") as handle:
                reader = csv.DictReader(handle)
                for row in reader:
                    name = (row.get("Name") or "").strip().lower()
                    if name == term_lower:
                        return SourcePayload(source="hscg", data=row)
        except Exception as exc:
            LOGGER.debug("HSCG CSV lookup failed for %s: %s", term, exc)
        return None

    def _fetch_ewg(self, term: str) -> Optional[SourcePayload]:
        api_key = EWG_API_KEY
        if not api_key:
            return None
        try:
            url = "https://api.ewg.org/skindeep/ingredients"
            params = {"search": term, "api_key": api_key}
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            blob = response.json()
            results = blob.get("results") or blob.get("ingredients") or []
            if not results:
                return None
            return SourcePayload(source="ewg", data=results[0])
        except Exception as exc:
            LOGGER.debug("EWG lookup failed for %s: %s", term, exc)
            return None

    def _fetch_tgsc(self, term: str) -> Optional[SourcePayload]:
        if TGSC_API_KEY:
            url = "https://www.thegoodscentscompany.com/api/ingredient"
            try:
                response = self.session.get(url, params={"name": term, "api_key": TGSC_API_KEY}, timeout=10)
                response.raise_for_status()
                blob = response.json()
                if blob:
                    return SourcePayload(source="tgsc", data=blob[0] if isinstance(blob, list) else blob)
            except Exception as exc:
                LOGGER.debug("TGSC API lookup failed for %s: %s", term, exc)
        if TGSC_CSV_PATH.exists():
            try:
                with TGSC_CSV_PATH.open(encoding="utf-8") as handle:
                    reader = csv.DictReader(handle)
                    for row in reader:
                        if (row.get("name") or "").strip().lower() == term.lower():
                            return SourcePayload(source="tgsc", data=row)
            except Exception as exc:
                LOGGER.debug("TGSC CSV lookup failed for %s: %s", term, exc)
        return None

    def _fetch_health_canada_nhp(self, term: str) -> Optional[SourcePayload]:
        if NHP_API_KEY:
            url = "https://health-products.canada.ca/api/natural-ingredient"
            try:
                params = {"lang": "en", "type": "contains", "term": term, "api_key": NHP_API_KEY}
                response = self.session.get(url, params=params, timeout=10)
                response.raise_for_status()
                blob = response.json()
                if blob:
                    return SourcePayload(source="health_canada_nhp", data=blob[0] if isinstance(blob, list) else blob)
            except Exception as exc:
                LOGGER.debug("Health Canada API lookup failed for %s: %s", term, exc)
        if NHP_JSON_PATH.exists():
            try:
                records: List[Dict[str, Any]] = json.loads(NHP_JSON_PATH.read_text(encoding="utf-8"))
                for record in records:
                    if (record.get("name") or "").strip().lower() == term.lower():
                        return SourcePayload(source="health_canada_nhp", data=record)
            except Exception as exc:
                LOGGER.debug("Health Canada JSON lookup failed for %s: %s", term, exc)
        return None


BROKER = IngredientSourceBroker()


def fetch_metadata(term: str) -> Dict[str, Any]:
    """Convenience wrapper for the rest of the builder."""
    if not term or not term.strip():
        return {}
    result = BROKER.gather(term.strip())
    if result:
        LOGGER.debug("Retrieved external metadata for %s from %s", term, list(result.keys()))
    return result
```

================================================================================
## 2. CONFIGURATION AND DATA FILES
================================================================================

### data_builder/README.md
```markdown
# Data Builder Toolkit

This folder contains the autonomous tooling that compiles the ingredient library which seeds BatchTrack.

## Components

| Module | Purpose |
| --- | --- |
| `ingredients/term_collector.py` | Finds the canonical ingredient terms plus the master list of physical forms (botanical, extract, essential oil, solutions, etc.). It ships with built-in exemplar ingredients, can optionally read any directory of seed JSON you point it to, and leverages the AI API to generate 5k+ bases. Every ingredient receives a 110 relevance score so the compiler can process the most impactful items first. |
| `ingredients/database_manager.py` | Manages the resumable SQLite queue (`compiler_state.db`) and persists per-term priority. |
| `ingredients/ai_worker.py` | Sends the "perfect prompt" to the model for one ingredient and validates the JSON payload (including category assignment, shelf-life-in-days, and optional form bypass flags for items like water/ice). |
| `ingredients/compiler.py` | Orchestrates the iterative build: grabs the next term, locks it, calls the AI worker, validates, writes `ingredients/output/ingredients/<slug>.json`, and updates lookup fi
```

### Sample terms.json structure
```json
[
  {
    "term": "Evening Primrose Oil",
    "priority": 8
  },
  {
    "term": "Epsom Salt", 
    "priority": 9
  },
  {
    "term": "Elemi Oil",
    "priority": 6
  }
]
```

================================================================================
## 3. OUTPUT EXAMPLES
================================================================================

### Sample Ingredient Output: bergamot.json
```json
{
  "data_quality": {
    "caveats": [
      "Photosensitizing properties may vary by extraction method"
    ],
    "confidence": 0.95
  },
  "ingredient": {
    "botanical_name": "Citrus bergamia",
    "cas_number": "8007-75-8",
    "category": "Essential Oils",
    "common_name": "Bergamot",
    "detailed_description": "Bergamot essential oil is extracted from the rind of the bergamot orange, a citrus fruit primarily cultivated in Southern Italy. The oil is renowned for its distinctive citrusy, slightly floral aroma and is widely used in perfumery, aromatherapy, and flavoring applications. Cold-pressed bergamot oil contains bergaptene, which can cause photosensitivity, while steam-distilled versions have reduced levels of this compound.",
    "documentation": {
      "last_verified": "2024-03-15",
      "references": [
        {
          "notes": "IFRA guidelines for safe usage levels",
          "title": "IFRA Standard 48th Amendment",
          "url": "https://ifrafragrance.org/standards"
        }
      ]
    },
    "inci_name": "Citrus Aurantium Bergamia Peel Oil",
    "items": [
      {
        "applications": [
          "Candle",
          "Cold Process Soap",
          "Hot Process Soap",
          "Lotion",
          "Perfume"
        ],
        "form_bypass": false,
        "function_tags": [
          "Fragrance"
        ],
        "item_name": "Bergamot Essential Oil (Cold Pressed)",
        "physical_form": "Essential Oil",
        "safety_tags": [
          "Photosensitizer"
        ],
        "sds_hazards": [
          "Allergen",
          "Photosensitizer"
        ],
        "shelf_life_days": 1095,
        "sourcing": {
          "certifications": [
            "Organic"
          ],
          "common_origins": [
            "Italy"
          ],
          "supply_risks": [
            "Weather dependent harvest"
          ],
          "sustainability_notes": "Sustainable cultivation practices in Calabria region"
        },
        "specifications": {
          "flash_point_celsius": 57,
          "usage_rate_percent": {
            "leave_on_max": 0.4,
            "rinse_off_max": 2
          }
        },
        "storage": {
          "special_instructions": "Keep away from light and heat",
          "temperature_celsius": {
            "max": 25,
            "min": 5
          }
        },
        "synonyms": [
          "Bergamot Oil Cold Pressed"
        ]
      },
      {
        "applications": [
          "Beverage",
          "Chocolate",
          "Confection"
        ],
        "form_bypass": false,
        "function_tags": [
          "Flavor",
          "Fragrance"
        ],
        "item_name": "Bergamot Essential Oil (Steam Distilled)",
        "physical_form": "Essential Oil",
        "safety_tags": [],
        "sds_hazards": [
          "Allergen"
        ],
        "shelf_life_days": 1095,
        "sourcing": {
          "certifications": [
            "Organic"
          ],
          "common_origins": [
            "Italy"
          ],
          "supply_risks": [
            "Weather dependent harvest"
          ],
          "sustainability_notes": "Sustainable cultivation practices in Calabria region"
        },
        "specifications": {
          "flash_point_celsius": 60,
          "usage_rate_percent": {
            "leave_on_max": 1,
            "rinse_off_max": 2
          }
        },
        "storage": {
          "special_instructions": "Keep away from light and heat",
          "temperature_celsius": {
            "max": 25,
            "min": 5
          }
        },
        "synonyms": [
          "Bergamot Oil Steam Distilled"
        ]
      }
    ],
    "origin": {
      "processing_methods": [
        "Cold Pressed",
        "Steam Distilled"
      ],
      "regions": [
        "Italy",
        "Ivory Coast",
        "Argentina",
        "Turkey"
      ],
      "source_material": "Bergamot orange peel"
    },
    "primary_functions": [
      "Fragrance",
      "Flavor"
    ],
    "regulatory_notes": [
      "IFRA safe",
      "Food grade",
      "Photosensitizer"
    ],
    "short_description": "A citrus essential oil known for its distinctive aroma and flavor.",
    "taxonomy": {
      "color_profile": [
        "Yellow",
        "Green"
      ],
      "compatible_processes": [
        "Cold Process Soap",
        "Hot Process Soap",
        "Melt & Pour Soap",
        "Lotion Making",
        "Candle Making",
        "Chocolate Making",
        "Beverage Flavoring"
      ],
      "incompatible_processes": [],
      "scent_profile": [
        "Citrus",
        "Floral",
        "Fresh"
      ],
      "texture_profile": [
        "Liquid"
      ]
    }
  }
}
```

### Sample Physical Forms Output: physical_forms.json
```json
[
  "Absolute",
  "Beads",
  "Block",
  "Buds",
  "Butter",
  "CO2 Extract",
  "Chips",
  "Cold Pressed Oil",
  "Concentrate",
  "Crystals",
  "Decoction",
  "Distillate",
  "Essential Oil",
  "Flakes",
  "Flour",
  "Gel",
  "Ground",
  "Hydrosol",
  "Infusion",
  "Juice",
  "Oleoresin",
  "Paste",
  "Pellets",
  "Powder",
  "Puree",
  "Refined Oil",
  "Resin",
  "Slices",
  "Stock Solution",
  "Syrup",
  "Tincture",
  "Wax",
  "Whole"
]
```

### Sample Taxonomies Output: taxonomies.json  
```json
{
  "applications": [
    "Bath Bomb",
    "Beverage",
    "Candle",
    "Chocolate",
    "Cold Process Soap",
    "Confection",
    "Hot Process Soap",
    "Lotion",
    "Perfume"
  ],
  "color_profile": [
    "Clear",
    "Green",
    "Pale Yellow",
    "Yellow"
  ],
  "compatible_processes": [
    "Beverage Flavoring",
    "Candle Making", 
    "Chocolate Making",
    "Cold Process Soap",
    "Hot Process Soap",
    "Lotion Making",
    "Melt & Pour Soap"
  ],
  "function_tags": [
    "Flavor",
    "Fragrance"
  ],
  "safety_tags": [
    "Photosensitizer"
  ],
  "scent_profile": [
    "Citrus",
    "Floral",
    "Fresh",
    "Nutty"
  ],
  "texture_profile": [
    "Liquid",
    "Viscous"
  ]
}
```

================================================================================
## 4. USAGE INSTRUCTIONS
================================================================================

### Running the System

1. **Set up environment variables:**
   ```bash
   export OPENAI_API_KEY="your-api-key-here"
   export OPENAI_MODEL="gpt-4-turbo-preview"
   export OPENAI_TEMPERATURE="0.1"
   export COMPILER_SLEEP_SECONDS="3"
   ```

2. **Generate ingredient terms:**
   ```bash
   python -m data_builder.ingredients.term_collector \
     --target-count 5000 \
     --batch-size 250 \
     --seed-root app/seeders/globallist/ingredients
   ```

3. **Compile ingredient data:**
   ```bash  
   python -m data_builder.ingredients.compiler \
     --max-ingredients 100 \
     --min-priority 5
   ```

4. **Check queue status:**
   ```python
   from data_builder.ingredients import database_manager
   print(database_manager.get_queue_summary())
   ```

### Key Features

- **Priority-based processing**: Higher priority ingredients get processed first
- **Resumable compilation**: SQLite queue tracks progress and handles failures
- **Consistent schema**: AI generates structured JSON matching your database models
- **Rich metadata**: Captures regulatory info, safety data, storage requirements
- **Lookup files**: Maintains master lists of physical forms and taxonomies
- **External sources**: Extensible system for enriching data from APIs/datasets

### Integration Points

The generated JSON files integrate with BatchTrack's seeding system:
- `app/seeders/seed_ingredients.py` processes the output files
- Database models in `app/models/ingredient_reference.py` store the data
- Global item library uses the structured ingredient metadata

This system provides the foundation for BatchTrack's comprehensive ingredient database, enabling makers to work with standardized, well-documented raw materials across all their formulation projects.
